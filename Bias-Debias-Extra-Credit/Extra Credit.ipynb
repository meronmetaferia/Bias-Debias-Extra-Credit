{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8b77a28",
   "metadata": {},
   "source": [
    "I used all the presidential speeches by Clinton and Obama during their presidetial careers. Data adopted from [Grammar Lab](http://www.thegrammarlab.com/?nor-portfolio=corpus-of-presidential-speeches-cops-and-a-clintontrump-corpus) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39e271bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.1.2-cp38-cp38-win_amd64.whl (24.0 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from gensim) (3.0.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from gensim) (1.20.1)\n",
      "Requirement already satisfied: Cython==0.29.23 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from gensim) (0.29.23)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from gensim) (1.6.2)\n",
      "Requirement already satisfied: requests in c:\\users\\meron\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2021.5.30)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\meron\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.8.1->gensim) (1.26.4)\n",
      "Installing collected packages: gensim\n",
      "Successfully installed gensim-4.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ff84ee37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#library import code copied from https://github.com/tolga-b/debiaswe\n",
    "from __future__ import print_function, division\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import debiaswe as dwe\n",
    "import debiaswe.we as we\n",
    "from debiaswe.we import WordEmbedding\n",
    "from debiaswe.data import load_professions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "15c38924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading spacy and tokenizer\n",
    "import spacy\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from collections import Counter\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from random import shuffle,uniform\n",
    "from math import e , pow , log\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "708a88cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using NLTK library for tokenizing all words from english text\n",
    "\n",
    "def clean_text(text):\n",
    "    \n",
    "    #text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    speech_date=[]\n",
    "    \n",
    "    try:\n",
    "        matches = datefinder.find_dates(text)\n",
    "        for date in matches:\n",
    "            speech_date.append(date)\n",
    "    except:\n",
    "        None\n",
    "    finally:\n",
    "        text = re.sub('[\\d\\n]', ' ', text)\n",
    "        return speech_date[0],text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "380707e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing speeches\n",
    "\n",
    "# The absolute path of the current directory\n",
    "path = 'speeches'\n",
    "import os\n",
    "\n",
    "dir_base_clinton = path + \"/clinton\"\n",
    "dir_base_obama =  path + \"/obama\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54a9bbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to read files\n",
    "\n",
    "def read_file(filename):\n",
    "    input_file_text = open(filename, encoding='latin1').read()\n",
    "    return input_file_text\n",
    "\n",
    "    \n",
    "def read_directory_files(directory):\n",
    "    file_texts = []\n",
    "    files = [f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    for f in files:\n",
    "        file_text = read_file(os.path.join(directory, f))\n",
    "        file_texts.append({\"file\":f, \"content\": file_text })\n",
    "    return file_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc93c9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#calling funtion to read the files\n",
    "text_corpus_obama = read_directory_files(dir_base_obama)\n",
    "text_corpus_clinton = read_directory_files(dir_base_clinton)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef9129bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting list to string\n",
    "clinton_corpus = ' '.join([str(elem) for elem in text_corpus_clinton])\n",
    "obama_corpus = ' '.join([str(elem) for elem in text_corpus_obama])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e83afca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "clinton_s = clinton_corpus.replace(\"\\n\", \" \")\n",
    "obama_s = obama_corpus.replace(\"\\n\", \" \")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "93ac503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting string to word2vec \n",
    "clinton_word_2vec = gensim.models.Word2Vec(clinton_s, min_count = 1, vector_size = 100, window = 5)\n",
    "obama_word_2vec = gensim.models.Word2Vec(obama_s, min_count = 1, vector_size = 100, window = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ef1015f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving clinton & obama word2vec files as text \n",
    "clinton_word_2vec.save(\"clinton.txt\")\n",
    "obama_word_2vec.save(\"obama.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc95fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b31f286a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "44c0ccab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through each sentence for obama in the file\n",
    "\n",
    "# clinton = []\n",
    "\n",
    "# for i in sent_tokenize(clinton_s):\n",
    "#     temp = []\n",
    "\n",
    "#     # tokenize the sentence into words\n",
    "#     for j in word_tokenize(i):\n",
    "#         temp.append(j.lower())\n",
    "\n",
    "#     clinton.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8223d33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iterate through each sentence for obama in the file\n",
    "\n",
    "# obama = []\n",
    "\n",
    "# for i in sent_tokenize(clinton_s):\n",
    "#     temp = []\n",
    "\n",
    "#     # tokenize the sentence into words\n",
    "#     for j in word_tokenize(i):\n",
    "#         temp.append(j.lower())\n",
    "\n",
    "#     obama.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6eafdf",
   "metadata": {},
   "source": [
    "**Loading data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f01dea4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Reading data from clinton.txt\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-e48b3673ca88>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# load google news word2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#code in this cell adopted from https://github.com/tolga-b/debiaswe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mE\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordEmbedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'clinton.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\OneDrive\\Desktop\\Fall 2021\\Ethics for Data Science\\A5_Building a Biased ML System\\debiaswe\\debiaswe\\we.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, fname)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf8'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m                     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 322\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    323\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x80 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "# load google news word2vec\n",
    "#code in this cell adopted from https://github.com/tolga-b/debiaswe\n",
    "E = WordEmbedding('clinton.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "97a4e460",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Word2Vec' object has no attribute 'diff'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-ac1108f57825>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mv_gender\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiff\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'she'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'he'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'Word2Vec' object has no attribute 'diff'"
     ]
    }
   ],
   "source": [
    "v_gender = model1.diff('she','he')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2506905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# analogies gender\n",
    "a_gender_2 = E.best_analogies_dist_thresh(v_gender_2)\n",
    "\n",
    "for (a,b,c) in a_gender_2:\n",
    "    print(a+\"-\"+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be452a17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
